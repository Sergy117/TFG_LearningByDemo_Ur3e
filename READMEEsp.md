Control de Brazo Rob√≥tico UR3e mediante Mapeo Cinem√°tico desde Pose Humana

üìñ Descripci√≥n General

Este repositorio contiene el c√≥digo fuente, la documentaci√≥n y las herramientas para el Trabajo de Fin de Grado (TFG) centrado en la teleoperaci√≥n de un brazo rob√≥tico UR3e en el espacio de articulaciones. El sistema traduce los movimientos de un operador humano, capturados por un sistema de visi√≥n est√©reo con MediaPipe, a comandos de joint states para el robot, evitando deliberadamente el uso de la cinem√°tica inversa (IK) en tiempo real para priorizar la velocidad y la eficiencia computacional.

El objetivo principal es desarrollar y validar un pipeline de mapeo cinem√°tico robusto que sirva como base para tareas de Aprendizaje por Demostraci√≥n (LfD), utilizando Primitivas de Movimiento Din√°mico (DMPs) para que el robot aprenda y reproduzca tareas complejas demostradas por un humano.

(Sugerencia: ¬°A√±ade aqu√≠ un GIF o una imagen del resultado final, como la de la comparaci√≥n de Matplotlib o un v√≠deo de la simulaci√≥n en RViz!)
![GIF de demostraci√≥n del proyecto](documentation/demo.gif)

‚ú® Caracter√≠sticas Principales

    Sistema de Visi√≥n Est√©reo: Utiliza dos c√°maras Intel RealSense para una reconstrucci√≥n 3D precisa de la escena.

    Estimaci√≥n de Pose Humana 3D: Implementa MediaPipe Pose para la detecci√≥n de 33 landmarks corporales en tiempo real.

    Detecci√≥n de Objetos Personalizada: Integra un modelo YOLOv8 entrenado a medida y exportado a formato ONNX para la detecci√≥n de las piezas de la tarea.

    Mapeo Cinem√°tico Humano-Robot: Desarrolla un "traductor" que convierte la pose humana en √°ngulos de articulaci√≥n del robot, resolviendo las diferencias morfol√≥gicas y de sistemas de coordenadas.

    Aprendizaje por Demostraci√≥n (LfD): Usa Primitivas de Movimiento Din√°mico (DMPs) para aprender trayectorias complejas a partir de m√∫ltiples demostraciones humanas.

    Control del Robot en ROS: Se integra con ROS Noetic y MoveIt! para el control seguro del robot UR3e, tanto en simulaci√≥n (Gazebo/RViz) como en el hardware real.

üèóÔ∏è Arquitectura del Sistema

El proyecto sigue una arquitectura modular donde cada componente tiene una responsabilidad clara. El flujo de datos es el siguiente:

C√°maras Est√©reo ‚Üí 1. Nodo de Visi√≥n (Captura y Triangulaci√≥n 3D) ‚Üí 2. Pipeline de Aprendizaje Offline (Traducci√≥n y Entrenamiento de DMPs) ‚Üí 3. Nodo de Control del Robot (Carga y Ejecuci√≥n de DMPs) ‚Üí Robot UR3e

    Nodo de Visi√≥n (vision_system/): Captura las im√°genes, detecta los landmarks humanos y los objetos, y guarda las trayectorias 3D de la demostraci√≥n en archivos .csv.

    Pipeline de Aprendizaje (learning_and_validation/): Carga las demostraciones, las traduce al espacio articular del robot usando la metodolog√≠a del informe t√©cnico, y entrena los modelos DMP, guard√°ndolos como archivos .pkl.

    Nodo de Control (robot_control/): Carga los DMPs aprendidos y los utiliza para generar y ejecutar trayectorias en el robot a trav√©s de MoveIt.

üìÅ Estructura del Repositorio

TFG_UR3e_Teleoperacion/
|
‚îú‚îÄ‚îÄ üìÑ README.md              # <-- Este archivo. La portada del proyecto.
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # <-- Dependencias principales de Python.
|
‚îú‚îÄ‚îÄ ü§ñ robot_control/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts de ROS para controlar el robot (ej. dmpplayer.py)
|
‚îú‚îÄ‚îÄ üëÅÔ∏è vision_system/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts para la captura de demostraciones (ej. main_script_final.py)
|
‚îú‚îÄ‚îÄ üéì learning_and_validation/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts para validar datos y entrenar los DMPs (ej. learn_dmp.py)
|
‚îú‚îÄ‚îÄ üõ†Ô∏è auxiliary_tools/
‚îÇ   ‚îú‚îÄ‚îÄ 1_camera_calibration/  # Scripts y README para la calibraci√≥n est√©reo.
‚îÇ   ‚îú‚îÄ‚îÄ 2_hand_eye_calibration/ # Scripts y README para la calibraci√≥n c√°mara-robot.
‚îÇ   ‚îî‚îÄ‚îÄ 3_yolo_training/     # Scripts y README para el entrenamiento del modelo YOLO.
|
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ calibration_files/   # Archivos .npy de las calibraciones.
‚îÇ   ‚îî‚îÄ‚îÄ learned_dmps/        # Archivos .pkl de los DMPs aprendidos.
‚îÇ
‚îî‚îÄ‚îÄ üìù documentation/
    ‚îî‚îÄ‚îÄ (Im√°genes, GIFs y otros recursos para la documentaci√≥n del TFG).

üöÄ Gu√≠a de Inicio R√°pido

Para replicar el proyecto, se debe seguir un proceso ordenado. Cada subcarpeta en auxiliary_tools y los directorios principales contienen su propio README.md con instrucciones detalladas.

    Clonar el Repositorio:
    Bash

    git clone [URL-de-tu-repositorio]
    cd TFG_UR3e_Teleoperacion

    Instalar Dependencias: Instala las librer√≠as necesarias especificadas en los diferentes archivos requirements.txt.

    Calibraci√≥n (Carpeta: auxiliary_tools/):

        Paso 1: Ejecuta el script de 1_camera_calibration para calibrar el sistema est√©reo.

        Paso 2: Ejecuta los scripts de 2_hand_eye_calibration para obtener la transformaci√≥n entre la c√°mara y el robot.

    Entrenamiento del Detector de Objetos (Carpeta: auxiliary_tools/):

        Sigue las instrucciones en 3_yolo_training/README.md para capturar im√°genes, etiquetarlas y entrenar tu modelo .onnx.

    Creaci√≥n del Dataset de Demostraciones (Carpeta: vision_system/):

        Ejecuta el script principal de visi√≥n para grabar un conjunto de demostraciones de la tarea.

    Aprendizaje del Modelo (Carpeta: learning_and_validation/):

        Usa el script de validaci√≥n para inspeccionar la calidad de tus demostraciones.

        Ejecuta el script learn_dmp.py para procesar el dataset y entrenar los DMPs.

    Ejecuci√≥n en el Robot (Carpeta: robot_control/):

        Lanza la simulaci√≥n en RViz o la conexi√≥n con el robot real.

        Ejecuta el script robot_player_from_dmp.py para que el robot reproduzca la tarea aprendida.

üë§ Autor

Sergio Gonz√°lez Rodr√≠guez

    Email: meanssergy@gmail.com

    GitHub: Sergy117

Trabajo de Fin de Grado para la Universidade de Santiago de Compostela (USC) - 2025.