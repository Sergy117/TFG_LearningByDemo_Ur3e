Aprendizaje por Demostraci√≥n (LfD): Control de Brazo Rob√≥tico UR3e mediante Mapeo Cinem√°tico desde Pose Humana

üìñ Descripci√≥n General

Este repositorio contiene el c√≥digo fuente, la documentaci√≥n y las herramientas para un Trabajo de Fin de Grado (TFG) centrado en la implementaci√≥n de un sistema de Aprendizaje por Demostraci√≥n (Learning from Demonstration - LfD) para un brazo rob√≥tico UR3e.

El n√∫cleo del proyecto es el desarrollo de un pipeline robusto que traduce los movimientos de un operador humano, capturados por un sistema de visi√≥n est√©reo, a trayectorias en el espacio articular del robot. Estas trayectorias se utilizan para entrenar un modelo generativo de la tarea basado en Primitivas de Movimiento Din√°mico (DMPs). El resultado es un sistema donde el robot puede aprender y reproducir de forma aut√≥noma habilidades motoras complejas a partir de la observaci√≥n.

(Sugerencia: ¬°A√±ade aqu√≠ un GIF o una imagen del resultado final!)
![GIF de demostraci√≥n del proyecto](documentation/demo.gif)

‚ú® Caracter√≠sticas Principales

    Aprendizaje por Demostraci√≥n (LfD) con DMPs: El sistema aprende habilidades motoras a partir de m√∫ltiples demostraciones humanas, generando trayectorias suaves y generalizables.

    Mapeo Cinem√°tico Humano-Robot: Un "traductor" cinem√°tico robusto que convierte la pose humana 3D a √°ngulos de articulaci√≥n del robot, resolviendo las diferencias morfol√≥gicas.

    Estimaci√≥n de Pose Humana 3D: Implementa MediaPipe Pose sobre un sistema de visi√≥n est√©reo para una detecci√≥n precisa de 33 landmarks corporales.

    Detecci√≥n de Objetos Personalizada: Integra un modelo YOLOv8 entrenado a medida (exportado a ONNX) para la percepci√≥n de objetos clave en la tarea.

    Control del Robot en ROS: Se integra con ROS Noetic y MoveIt! para el control seguro del robot UR3e en el espacio de articulaciones, tanto en simulaci√≥n como en hardware real.

üèóÔ∏è Arquitectura del Sistema

El proyecto sigue una arquitectura modular donde cada componente tiene una responsabilidad clara. El flujo de datos es el siguiente:

C√°maras Est√©reo ‚Üí 1. Nodo de Visi√≥n (Captura de Demos) ‚Üí 2. Pipeline de Aprendizaje Offline (Traducci√≥n y Entrenamiento de DMPs) ‚Üí 3. Nodo de Control del Robot (Carga y Ejecuci√≥n de DMPs) ‚Üí Robot UR3e

    Nodo de Visi√≥n (vision_system/): Captura las demostraciones humanas y las guarda como trayectorias 3D en archivos .csv.

    Pipeline de Aprendizaje (learning_and_validation/): Es el coraz√≥n del sistema LfD. Carga las demostraciones, las traduce al espacio articular del robot y entrena los modelos DMP, guard√°ndolos como archivos .pkl.

    Nodo de Control (robot_control/): Carga los DMPs ya entrenados y los utiliza para generar y ejecutar la tarea de forma aut√≥noma en el robot.

üìÅ Estructura del Repositorio

TFG_UR3e_Teleoperation/
|
‚îú‚îÄ‚îÄ üìÑ README.md              # <-- Este archivo. La portada del proyecto.
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # <-- Dependencias principales de Python.
|
‚îú‚îÄ‚îÄ ü§ñ robot_control/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts de ROS para controlar el robot (ej. dmpplayer.py)
|
‚îú‚îÄ‚îÄ üëÅÔ∏è vision_system/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts para la captura de demostraciones (ej. main_script_final.py)
|
‚îú‚îÄ‚îÄ üéì learning_and_validation/
‚îÇ   ‚îî‚îÄ‚îÄ src/                 # Scripts para validar datos y entrenar los DMPs (ej. learn_dmp.py)
|
‚îú‚îÄ‚îÄ üõ†Ô∏è auxiliary_tools/
‚îÇ   ‚îú‚îÄ‚îÄ 1_camera_calibration/  # Scripts y README para la calibraci√≥n est√©reo.
‚îÇ   ‚îú‚îÄ‚îÄ 2_hand_eye_calibration/ # Scripts y README para la calibraci√≥n c√°mara-robot.
‚îÇ   ‚îî‚îÄ‚îÄ 3_yolo_training/     # Scripts y README para el entrenamiento del modelo YOLO.
|
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ calibration_files/   # Archivos .npy de las calibraciones.
‚îÇ   ‚îî‚îÄ‚îÄ learned_dmps/        # Archivos .pkl de los DMPs aprendidos.
‚îÇ
‚îî‚îÄ‚îÄ üìù documentation/
    ‚îî‚îÄ‚îÄ (Im√°genes, GIFs y otros recursos para la documentaci√≥n del TFG).

üöÄ Gu√≠a de Inicio R√°pido

Para replicar el proyecto, se debe seguir un proceso ordenado. Cada subcarpeta contiene su propio README.md con instrucciones detalladas.

    Clonar el Repositorio:
    Bash

    git clone [URL-de-tu-repositorio]
    cd TFG_UR3e_Teleoperation

    Instalar Dependencias: Instala las librer√≠as necesarias especificadas en los diferentes archivos requirements.txt.

    Calibraci√≥n (Ver auxiliary_tools/):

        Paso 1: Ejecuta el script de 1_camera_calibration.

        Paso 2: Ejecuta los scripts de 2_hand_eye_calibration.

    Entrenamiento del Detector de Objetos (Ver auxiliary_tools/):

        Sigue las instrucciones en 3_yolo_training/README.md para entrenar tu modelo .onnx.

    Creaci√≥n del Dataset de Demostraciones (Ver vision_system/):

        Ejecuta el script principal de visi√≥n para grabar un conjunto de demostraciones.

    Aprendizaje del Modelo (Ver learning_and_validation/):

        Usa el script de validaci√≥n para inspeccionar la calidad de tus demostraciones.

        Ejecuta el script learn_dmp.py para procesar el dataset y entrenar los DMPs.

    Ejecuci√≥n en el Robot (Ver robot_control/):

        Lanza la simulaci√≥n en RViz o la conexi√≥n con el robot real.

        Ejecuta el script robot_player_from_dmp.py para que el robot reproduzca la tarea aprendida.

üë§ Autor

Sergio Gonz√°lez Rodr√≠guez

    Email: meanssergy@gmail.com

    GitHub: Sergy117

Trabajo de Fin de Grado para la Universidade de Santiago de Compostela (USC) - 2025.
